# Configuration for Summarization Tasks

extractive:
  dataset_name: "scotus_dataset" # Corrected dataset name
  dataset_split: "test"           # Split to use for running summarization
  num_summary_sentences: 5        # Number of sentences for TextRank summary
  min_sentence_length: 5          # Minimum words for a sentence to be considered

# --- Configuration for Abstractive Summarization ---
abstractive:
  # Dataset Configuration
  dataset_name: "ChicagoHAI/CaseSumm" # Hugging Face dataset name
  dataset_split: "train"             # Corrected: Use the available 'train' split
  text_column: "opinion"             # Column containing the source text
  summary_column: "syllabus"          # Corrected: Column containing the target/reference summary

  # Model Configuration
  model_name: "t5_small_casesumm_abstractive" # Name for potential fine-tuned saving
  base_model: "models/summarization/t5_small_casesumm_finetuned" # Updated path to load fine-tuned model
  path_to_finetuned_model: "models/summarization/t5_small_casesumm_finetuned" # Optional: Path if loading a previously fine-tuned model

  # Tokenizer/Generation Configuration
  max_input_length: 1024 # Max tokens for T5 encoder (longer sequences will be truncated)
  max_target_length: 150 # Max tokens for the generated summary
  min_target_length: 40  # Min tokens for the generated summary

  # Inference Parameters (adjust as needed)
  batch_size: 24         # Increased from 8: Batch size for generating summaries
  num_beams: 4           # Beam search width
  length_penalty: 2.0    # Penalize longer sequences slightly
  no_repeat_ngram_size: 3 # Prevent repeating trigrams

  # --- Training Configuration ---
  training:
    output_dir: "models/summarization/t5_small_casesumm_finetuned" # Directory to save the fine-tuned model & checkpoints
    num_train_epochs: 3            # Number of training epochs
    per_device_train_batch_size: 8 # Increased from 4
    per_device_eval_batch_size: 8  # Batch size for evaluation (if using eval_split)
    # eval_split: "validation"     # Optional: Name of validation split if available in dataset
    learning_rate: 5e-5          # Initial learning rate
    weight_decay: 0.01           # Weight decay for regularization
    #gradient_accumulation_steps: 4 # Accumulate gradients over 4 steps (effective batch size = 4 * 4 = 16)
    gradient_accumulation_steps: 2
    lr_scheduler_type: "linear"    # Learning rate scheduler type
    num_warmup_steps: 0            # Number of warmup steps for scheduler
    logging_steps: 50            # Log training loss every 50 steps
    save_steps: 500             # Save a checkpoint every 500 steps
    save_total_limit: 2          # Keep only the last 2 checkpoints
    predict_with_generate: True  # Needed for Seq2Seq metrics like ROUGE during evaluation
    fp16: True                   # Use mixed precision (if GPU supports it, requires accelerate)

  # Paths (Optional - for fine-tuning)
  # paths:
  #   output_dir: "models/summarization/{model_name}" # Where to save fine-tuned models/checkpoints

# --- Placeholders for Abstractive Summarization ---
# abstractive:
#   paths:
#     input_data: data/processed/summarization_data # Example path for training data
#     output_models: models/summarization/{model_name} # Where to save trained models

#   model:
#     name: abstractive_summarizer_v1 # Name for saving the fine-tuned model
#     base_model: google-t5/t5-small # Example base model for fine-tuning

#   training:
#     num_epochs: 3
#     batch_size: 4
#     learning_rate: 5e-5

#   generation_params: # Parameters for generating summaries with the trained model
#     max_length: 150  # Max tokens in the generated summary
#     min_length: 40   # Min tokens in the generated summary
#     num_beams: 4     # Beam search width
#     length_penalty: 2.0 # Penalize longer sequences
#     no_repeat_ngram_size: 3 # Prevent repeating trigrams

# paths:
#   input_data: ...
#   output_models: models/summarization/{model_name}

# model:
#   name: summarization_model_v1
#   base_model: google/pegasus-large # Example

# training:
#   num_epochs: ...
#   batch_size: ...
#   learning_rate: ...

# generation_params:
#   max_length: 512
#   min_length: 50
#   num_beams: 4 