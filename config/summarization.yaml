# Configuration for Legal Document Summarization

# Extractive Summarization Settings
extractive:
  dataset_name: "scotus_dataset"  # Dataset to use for extractive summarization
  dataset_split: "test"           # Which split of the dataset to use
  num_summary_sentences: 5        # Number of sentences to extract
  min_sentence_length: 5          # Minimum words per sentence to consider

# Abstractive Summarization Settings
abstractive:
  dataset_name: "ChicagoHAI/CaseSumm"  # Legal case dataset with opinions and syllabi
  dataset_split: "train"               # Dataset split to use
  text_column: "opinion"               # Column containing the source text
  summary_column: "syllabus"           # Column containing the reference summary
  base_model: "nsi319/legal-pegasus"   # Specialized model for legal summarization
  max_input_length: 1024               # Maximum input tokens
  max_target_length: 256               # Maximum summary tokens
  batch_size: 4                        # Batch size for inference
  num_beams: 8                         # Beam search width (higher = better quality, slower)
  length_penalty: 2.0                  # Controls summary length (higher = longer)
  min_length: 50                       # Minimum tokens in generated summary
  no_repeat_ngram_size: 2              # Prevents repetition of phrases
  early_stopping: true                 # Stop generation when all beams finished
  path_to_finetuned_model: "./models/legal-pegasus-casesumm"

# Fine-tuning Configuration
fine_tuning:
  dataset_name: "ChicagoHAI/CaseSumm"  # Dataset for fine-tuning
  text_column: "opinion"               # Input text column name
  summary_column: "syllabus"           # Target summary column name
  base_model: "nsi319/legal-pegasus"   # Starting model to fine-tune
  max_input_length: 512                # Maximum input length (reduced for faster training)
  max_target_length: 128               # Maximum target length 
  batch_size: 4                        # Batch size per device
  max_train_samples: 600               # Limit training samples for faster iteration
  learning_rate: 3e-5                  # Adam learning rate
  weight_decay: 0.01                   # L2 regularization
  num_epochs: 2                        # Number of training epochs
  gradient_accumulation_steps: 2       # Accumulate gradients over multiple steps
  fp16: true                           # Use mixed precision training (faster)
  save_strategy: "steps"               # When to save checkpoints
  save_steps: 200                      # Save every N steps
  evaluation_strategy: "steps"         # When to run evaluation
  eval_steps: 200                      # Evaluate every N steps
  output_dir: "./models/legal-pegasus-casesumm"  # Where to save model
  logging_dir: "./logs/legal-pegasus-casesumm"   # Training logs location

