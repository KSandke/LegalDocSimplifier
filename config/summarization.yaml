# Configuration for Legal Document Summarization

# Extractive Summarization Settings
extractive:
  dataset_name: "scotus_dataset"  # Dataset to use for extractive summarization
  dataset_split: "test"           # Which split of the dataset to use
  num_summary_sentences: 5        # Number of sentences to extract
  min_sentence_length: 5          # Minimum words per sentence to consider

# Abstractive Summarization Settings
abstractive:
  dataset_name: "ChicagoHAI/CaseSumm"  # Legal case dataset with opinions and syllabi
  dataset_split: "train"               # Dataset split to use
  text_column: "opinion"               # Column containing the source text
  summary_column: "syllabus"           # Column containing the reference summary
  base_model: "nsi319/legal-pegasus"   # Specialized model for legal summarization
  max_input_length: 1024               # Maximum input tokens
  max_target_length: 256               # Maximum summary tokens
  batch_size: 4                        # Batch size for inference
  num_beams: 8                         # Beam search width (higher = better quality, slower)
  length_penalty: 2.0                  # Controls summary length (higher = longer)
  min_length: 50                       # Minimum tokens in generated summary
  no_repeat_ngram_size: 2              # Prevents repetition of phrases
  early_stopping: true                 # Stop generation when all beams finished
  # path_to_finetuned_model: "./models/legal-pegasus-scotus/checkpoint-375"

# Fine-tuning Configuration
fine_tuning:
  dataset_name: "ChicagoHAI/CaseSumm"  # Dataset for fine-tuning
  text_column: "opinion"               # Input text column name
  summary_column: "syllabus"           # Target summary column name
  base_model: "nsi319/legal-pegasus"   # Starting model to fine-tune
  max_input_length: 512                # Maximum input length (reduced for faster training)
  max_target_length: 128               # Maximum target length 
  batch_size: 4                        # Batch size per device
  max_train_samples: 500               # Limit training samples for faster iteration
  learning_rate: 3e-5                  # Adam learning rate
  weight_decay: 0.01                   # L2 regularization
  num_epochs: 2                        # Number of training epochs
  gradient_accumulation_steps: 2       # Accumulate gradients over multiple steps
  fp16: true                           # Use mixed precision training (faster)
  save_strategy: "steps"               # When to save checkpoints
  save_steps: 100                      # Save every N steps
  evaluation_strategy: "steps"         # When to run evaluation
  eval_steps: 200                      # Evaluate every N steps
  output_dir: "./models/legal-pegasus-casesumm"  # Where to save model
  logging_dir: "./logs/legal-pegasus-casesumm"   # Training logs location

# --- Configuration for Abstractive Summarization ---
# abstractive:
#   # Dataset Configuration
#   dataset_name: "ChicagoHAI/CaseSumm" # Hugging Face dataset name
#   dataset_split: "train"             # Corrected: Use the available 'train' split
#   text_column: "opinion"             # Column containing the source text
#   summary_column: "syllabus"          # Corrected: Column containing the target/reference summary
#
#   # Model Configuration
#   model_name: "t5_small_casesumm_abstractive" # Name for potential fine-tuned saving
#   base_model: "models/summarization/t5_small_casesumm_finetuned" # Updated path to load fine-tuned model
#   path_to_finetuned_model: "models/summarization/t5_small_casesumm_finetuned" # Optional: Path if loading a previously fine-tuned model
#
#   # Tokenizer/Generation Configuration
#   max_input_length: 1024 # Max tokens for T5 encoder (longer sequences will be truncated)
#   max_target_length: 150 # Max tokens for the generated summary
#   min_target_length: 40  # Min tokens for the generated summary
#
#   # Inference Parameters (adjust as needed)
#   batch_size: 24         # Increased from 8: Batch size for generating summaries
#   num_beams: 4           # Beam search width
#   length_penalty: 2.0    # Penalize longer sequences slightly
#   no_repeat_ngram_size: 3 # Prevent repeating trigrams
#
#   # --- Training Configuration ---
#   training:
#     output_dir: "models/summarization/t5_small_casesumm_finetuned" # Directory to save the fine-tuned model & checkpoints
#     num_train_epochs: 3            # Number of training epochs
#     per_device_train_batch_size: 8 # Increased from 4
#     per_device_eval_batch_size: 8  # Batch size for evaluation (if using eval_split)
#     # eval_split: "validation"     # Optional: Name of validation split if available in dataset
#     learning_rate: 5e-5          # Initial learning rate
#     weight_decay: 0.01           # Weight decay for regularization
#     #gradient_accumulation_steps: 4 # Accumulate gradients over 4 steps (effective batch size = 4 * 4 = 16)
#     gradient_accumulation_steps: 2
#     lr_scheduler_type: "linear"    # Learning rate scheduler type
#     num_warmup_steps: 0            # Number of warmup steps for scheduler
#     logging_steps: 50            # Log training loss every 50 steps
#     save_steps: 500             # Save a checkpoint every 500 steps
#     save_total_limit: 2          # Keep only the last 2 checkpoints
#     predict_with_generate: True  # Needed for Seq2Seq metrics like ROUGE during evaluation
#     fp16: True                   # Use mixed precision (if GPU supports it, requires accelerate)
#
#   # Paths (Optional - for fine-tuning)
#   # paths:
#   #   output_dir: "models/summarization/{model_name}" # Where to save fine-tuned models/checkpoints
#
# # --- Placeholders for Abstractive Summarization ---
# # abstractive:
# #   paths:
# #     input_data: data/processed/summarization_data # Example path for training data
# #     output_models: models/summarization/{model_name} # Where to save trained models
#
# #   model:
# #     name: abstractive_summarizer_v1 # Name for saving the fine-tuned model
# #     base_model: google-t5/t5-small # Example base model for fine-tuning
#
# #   training:
# #     num_epochs: 3
# #     batch_size: 4
# #     learning_rate: 5e-5
#
# #   generation_params: # Parameters for generating summaries with the trained model
# #     max_length: 150  # Max tokens in the generated summary
# #     min_length: 40   # Min tokens in the generated summary
# #     num_beams: 4     # Beam search width
# #     length_penalty: 2.0 # Penalize longer sequences
# #     no_repeat_ngram_size: 3 # Prevent repeating trigrams
#
# # paths:
# #   input_data: ...
# #   output_models: models/summarization/{model_name}
#
# # model:
# #   name: summarization_model_v1
# #   base_model: google/pegasus-large # Example
#
# # training:
# #   num_epochs: ...
# #   batch_size: ...
# #   learning_rate: ...
#
# # generation_params:
# #   max_length: 512
# #   min_length: 50
# #   num_beams: 4 