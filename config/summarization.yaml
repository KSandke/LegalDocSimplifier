# Configuration for Summarization Tasks

# Extractive Summarization Configuration
extractive:
  dataset_name: "scotus_dataset"
  dataset_split: "test"
  num_summary_sentences: 5
  min_sentence_length: 5

# Abstractive Summarization Configuration
abstractive:
  dataset_name: "scotus_dataset"
  dataset_split: "test"
  text_column: "text"
  summary_column: "label"
  base_model: "google/pegasus-cnn_dailymail"
  max_input_length: 1024
  max_target_length: 256
  batch_size: 4
  num_beams: 5
  length_penalty: 2.0
  min_length: 100
  no_repeat_ngram_size: 3
  early_stopping: true
  # Uncomment below line after fine-tuning
  # path_to_finetuned_model: "./models/pegasus-legal-summarizer"

# Fine-tuning Configuration
fine_tuning:
  dataset_name: "ChicagoHAI/CaseSumm"
  text_column: "opinion"
  summary_column: "syllabus"
  base_model: "google/pegasus-cnn_dailymail"
  max_input_length: 512  # Reduced from 1024 for faster training
  max_target_length: 128  # Reduced from 256 for faster training
  batch_size: 2  # Keeping small batch size for memory efficiency
  max_train_samples: 5000  # Use only 5000 examples for faster training
  learning_rate: 5e-5
  weight_decay: 0.01
  num_epochs: 3  # Reduced from 4 for faster training
  gradient_accumulation_steps: 4  # Add gradient accumulation for stability
  eval_steps: 1000  # Evaluate less frequently
  fp16: true  # Ensure mixed precision is enabled
  output_dir: "./models/pegasus-legal-summarizer"
  logging_dir: "./logs"

# --- Configuration for Abstractive Summarization ---
# abstractive:
#   # Dataset Configuration
#   dataset_name: "ChicagoHAI/CaseSumm" # Hugging Face dataset name
#   dataset_split: "train"             # Corrected: Use the available 'train' split
#   text_column: "opinion"             # Column containing the source text
#   summary_column: "syllabus"          # Corrected: Column containing the target/reference summary
#
#   # Model Configuration
#   model_name: "t5_small_casesumm_abstractive" # Name for potential fine-tuned saving
#   base_model: "models/summarization/t5_small_casesumm_finetuned" # Updated path to load fine-tuned model
#   path_to_finetuned_model: "models/summarization/t5_small_casesumm_finetuned" # Optional: Path if loading a previously fine-tuned model
#
#   # Tokenizer/Generation Configuration
#   max_input_length: 1024 # Max tokens for T5 encoder (longer sequences will be truncated)
#   max_target_length: 150 # Max tokens for the generated summary
#   min_target_length: 40  # Min tokens for the generated summary
#
#   # Inference Parameters (adjust as needed)
#   batch_size: 24         # Increased from 8: Batch size for generating summaries
#   num_beams: 4           # Beam search width
#   length_penalty: 2.0    # Penalize longer sequences slightly
#   no_repeat_ngram_size: 3 # Prevent repeating trigrams
#
#   # --- Training Configuration ---
#   training:
#     output_dir: "models/summarization/t5_small_casesumm_finetuned" # Directory to save the fine-tuned model & checkpoints
#     num_train_epochs: 3            # Number of training epochs
#     per_device_train_batch_size: 8 # Increased from 4
#     per_device_eval_batch_size: 8  # Batch size for evaluation (if using eval_split)
#     # eval_split: "validation"     # Optional: Name of validation split if available in dataset
#     learning_rate: 5e-5          # Initial learning rate
#     weight_decay: 0.01           # Weight decay for regularization
#     #gradient_accumulation_steps: 4 # Accumulate gradients over 4 steps (effective batch size = 4 * 4 = 16)
#     gradient_accumulation_steps: 2
#     lr_scheduler_type: "linear"    # Learning rate scheduler type
#     num_warmup_steps: 0            # Number of warmup steps for scheduler
#     logging_steps: 50            # Log training loss every 50 steps
#     save_steps: 500             # Save a checkpoint every 500 steps
#     save_total_limit: 2          # Keep only the last 2 checkpoints
#     predict_with_generate: True  # Needed for Seq2Seq metrics like ROUGE during evaluation
#     fp16: True                   # Use mixed precision (if GPU supports it, requires accelerate)
#
#   # Paths (Optional - for fine-tuning)
#   # paths:
#   #   output_dir: "models/summarization/{model_name}" # Where to save fine-tuned models/checkpoints
#
# # --- Placeholders for Abstractive Summarization ---
# # abstractive:
# #   paths:
# #     input_data: data/processed/summarization_data # Example path for training data
# #     output_models: models/summarization/{model_name} # Where to save trained models
#
# #   model:
# #     name: abstractive_summarizer_v1 # Name for saving the fine-tuned model
# #     base_model: google-t5/t5-small # Example base model for fine-tuning
#
# #   training:
# #     num_epochs: 3
# #     batch_size: 4
# #     learning_rate: 5e-5
#
# #   generation_params: # Parameters for generating summaries with the trained model
# #     max_length: 150  # Max tokens in the generated summary
# #     min_length: 40   # Min tokens in the generated summary
# #     num_beams: 4     # Beam search width
# #     length_penalty: 2.0 # Penalize longer sequences
# #     no_repeat_ngram_size: 3 # Prevent repeating trigrams
#
# # paths:
# #   input_data: ...
# #   output_models: models/summarization/{model_name}
#
# # model:
# #   name: summarization_model_v1
# #   base_model: google/pegasus-large # Example
#
# # training:
# #   num_epochs: ...
# #   batch_size: ...
# #   learning_rate: ...
#
# # generation_params:
# #   max_length: 512
# #   min_length: 50
# #   num_beams: 4 